<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-D022F6NT2P"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-D022F6NT2P')</script><script>(function(b,d,e,a,g){b[a]=b[a]||[],b[a].push({'gtm.start':(new Date).getTime(),event:'gtm.js'});var f=d.getElementsByTagName(e)[0],c=d.createElement(e),h=a!='dataLayer'?'&l='+a:'';c.async=!0,c.src='https://www.googletagmanager.com/gtm.js?id='+g+h,f.parentNode.insertBefore(c,f)})(window,document,'script','dataLayer','GTM-P72R45W')</script><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=https://koktlzz.github.io/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://koktlzz.github.io/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://koktlzz.github.io/fonts/KaTeX_Main-Regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://koktlzz.github.io/fonts/KaTeX_Math-Italic.woff2 type=font/woff2 crossorigin><link rel=stylesheet href=https://koktlzz.github.io/main.6fd70af9369bef8ad98be8ac549b09d4cc31a7751f59f66dd296a20fd0fa4ae07ec51aee16a7e125439572da86f49095224d7d27d5f3dab10a0fa43783ecd2e2.css integrity="sha512-b9cK+Tab74rZi+isVJsJ1Mwxp3UfWfZt0paiD9D6SuB+xRruFqfhJUOVctqG9JCVIk19J9Xz2rEKD6Q3g+zS4g==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Open vSwitch | Inspire Hub</title><meta name=description content><link rel=canonical href=https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="Open vSwitch"><meta property="og:description" content="前言 Openshift SDN 是由 Overlay 网络 OVS（Open vSwitch）建立的，其使用的插件如下：
 ovs-subnet: 默认插件，提供一个扁平化的 Pod 网络以实现 Pod 与其他任何 Pod 或 Service 的通信； ovs-multitenant：实现多租户管理，隔离不同 Project 之间的网络通信。每个 Project 都有一个 NETID（即 VxLAN 中的 VNID），可以使用 oc get netnamspaces 命令查看； ovs-networkpolicy：基于 Kubernetes 中的 NetworkPolicy 资源实现网络策略管理。  在 Openshift 集群中的节点上，有以下几个网络设备：
 br0：OpenShift 创建和管理的 OVS 网桥，它会使用 OpenFlow 流表来实现数据包的转发和隔离； vxlan0：VxLAN 隧道端点，即 VTEP（Virtual Tunnel End Point），用于集群内部 Pod 之间的通信； tun0：节点上所有 Pod 的默认网关，用于 Pod 与集群外部和 Pod 与 Service 之间的通信； veth：Pod 通过veth-pair连接到br0网桥的端点。  ovs-ofctl -O OpenFlow13 show br0 命令可以查看br0上的所有端口及其编号："><meta property="og:url" content="https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/"><meta property="og:site_name" content="Inspire Hub"><meta property="article:published_time" content="2020-11-04T09:19:42+01:00"><meta property="article:modified_time" content="2020-11-04T09:19:42+01:00"><meta property="og:image" content="https://koktlzz.github.io/doks.png"><meta property="og:image:alt" content="Inspire Hub"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@getdoks"><meta name=twitter:creator content="@henkverlinde"><meta name=twitter:title content="Open vSwitch"><meta name=twitter:description content><meta name=twitter:image content="https://koktlzz.github.io/doks.png"><meta name=twitter:image:alt content="Open vSwitch"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://koktlzz.github.io/#/schema/organization/1","name":"Doks","url":"https://koktlzz.github.io/","sameAs":["https://twitter.com/getdoks","https://github.com/h-enk/doks"],"logo":{"@type":"ImageObject","@id":"https://koktlzz.github.io/#/schema/image/1","url":"https://koktlzz.github.io/logo-doks.png","width":512,"height":512,"caption":"Doks"},"image":{"@id":"https://koktlzz.github.io/#/schema/image/1"}},{"@type":"WebSite","@id":"https://koktlzz.github.io/#/schema/website/1","url":"https://koktlzz.github.io/","name":"Inspire Hub","description":"","publisher":{"@id":"https://koktlzz.github.io/#/schema/organization/1"}},{"@type":"WebPage","@id":"https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/","url":"https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/","name":"Open vSwitch","description":"","isPartOf":{"@id":"https://koktlzz.github.io/#/schema/website/1"},"about":{"@id":"https://koktlzz.github.io/#/schema/organization/1"},"datePublished":"2020-11-04T09:19:42CET","dateModified":"2020-11-04T09:19:42CET","breadcrumb":{"@id":"https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/"]}]},{"@type":"BreadcrumbList","@id":"https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://koktlzz.github.io/","url":"https://koktlzz.github.io/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://koktlzz.github.io/kubernetes/","url":"https://koktlzz.github.io/kubernetes/","name":"Kubernetes"}},{"@type":"ListItem","position":3,"item":{"@type":"WebPage","@id":"https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/","url":"https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/","name":"Kubernetes% E8% Bf%9 B% E9%98% B6"}},{"@type":"ListItem","position":4,"item":{"@id":"https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/#/schema/image/2","url":"https://koktlzz.github.io/doks.png","contentUrl":"https://koktlzz.github.io/doks.png","caption":"Open vSwitch"}]}]}</script><meta name=theme-color content="#fff"><link rel=apple-touch-icon sizes=180x180 href=https://koktlzz.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://koktlzz.github.io/favicon-32x32.ico><link rel=icon type=image/png sizes=16x16 href=https://koktlzz.github.io/favicon-16x16.ico><link rel=manifest crossorigin=use-credentials href=https://koktlzz.github.io/site.webmanifest></head><body class="Kubernetes single"><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-P72R45W" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><div class="header-bar fixed-top"></div><header class="navbar fixed-top navbar-expand-md navbar-light"><div class=container-fluid><input class="menu-btn order-0" type=checkbox id=menu-btn>
<label class="menu-icon d-md-none" for=menu-btn><span class=navicon></span></label><a class="navbar-brand order-1 order-md-0 me-auto" href=https://koktlzz.github.io/>Inspire Hub</a>
<button id=mode class="btn btn-link order-2 order-md-4" type=button aria-label="Toggle mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button><ul class="navbar-nav social-nav order-3 order-md-5"><li class=nav-item><a class=nav-link href=https://github.com/koktlzz><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><span class="ms-2 visually-hidden">GitHub</span></a></li></ul><div class="collapse navbar-collapse order-4 order-md-1"><ul class="navbar-nav main-nav me-auto order-5 order-md-2"><li class=nav-item><a class=nav-link href=https://koktlzz.github.io/docker/docker%E5%9F%BA%E7%A1%80/intro>Docker</a></li><li class=nav-item><a class=nav-link href=https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/intro>Kubernetes</a></li><li class=nav-item><a class=nav-link href=https://koktlzz.github.io/elastic/elasticstack/intro>Elastic</a></li><li class=nav-item><a class=nav-link href=https://koktlzz.github.io/go/go%E5%9F%BA%E7%A1%80/intro>Go</a></li><li class=nav-item><a class=nav-link href=https://koktlzz.github.io/infra/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E5%9F%BA%E7%A1%80/tcpip%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B>Infra</a></li></ul><div class="break order-6 d-md-none"></div><form class="navbar-form flex-grow-1 order-7 order-md-3"><input id=userinput class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded"></div></form></div></div></header><div class="wrap container-fluid" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-3 col-xl-2 d-none d-xl-block"><nav class=docs-links aria-label="Main navigation"><h3>Kubernetes 基础</h3><ul class=list-unstyled><li><a class=docs-link href=https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/intro/>Get Started</a></li><li><a class=docs-link href=https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/infrastructure/>Infrastructure</a></li><li><a class=docs-link href=https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/pod%E4%B8%8Enamespace/>Pod 与 Namespace</a></li><li><a class=docs-link href=https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/service/>Service</a></li><li><a class=docs-link href=https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/volume/>Volume</a></li><li><a class=docs-link href=https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/workloads/>Workloads</a></li></ul><h3>Kubernetes 进阶</h3><ul class=list-unstyled><li><a class=docs-link href=https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/networkpolicy/>Network Policy</a></li><li><a class=docs-link href=https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/ingress/>Ingress</a></li><li><a class=docs-link href=https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/sdn/>SDN</a></li><li><a class=docs-link href=https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/>Open vSwitch</a></li></ul></nav></div><main class="docs-content col-lg-13 col-xl-11"><h1>Open vSwitch</h1><p class=lead></p><h2 id=前言>前言<a href=#前言 class=anchor aria-hidden=true>#</a></h2><p>Openshift SDN 是由 Overlay 网络 OVS（Open vSwitch）建立的，其使用的插件如下：</p><ul><li>ovs-subnet: 默认插件，提供一个扁平化的 Pod 网络以实现 Pod 与其他任何 Pod 或 Service 的通信；</li><li>ovs-multitenant：实现多租户管理，隔离不同 Project 之间的网络通信。每个 Project 都有一个 NETID（即 VxLAN 中的 VNID），可以使用 <strong>oc get netnamspaces</strong> 命令查看；</li><li>ovs-networkpolicy：基于 Kubernetes 中的 NetworkPolicy 资源实现网络策略管理。</li></ul><p>在 Openshift 集群中的节点上，有以下几个网络设备：</p><ul><li><code>br0</code>：OpenShift 创建和管理的 OVS 网桥，它会使用 OpenFlow 流表来实现数据包的转发和隔离；</li><li><code>vxlan0</code>：VxLAN 隧道端点，即 VTEP（Virtual Tunnel End Point），用于集群内部 Pod 之间的通信；</li><li><code>tun0</code>：节点上所有 Pod 的默认网关，用于 Pod 与集群外部和 Pod 与 Service 之间的通信；</li><li><code>veth</code>：Pod 通过<code>veth-pair</code>连接到<code>br0</code>网桥的端点。</li></ul><p><strong>ovs-ofctl -O OpenFlow13 show br0</strong> 命令可以查看<code>br0</code>上的所有端口及其编号：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#ff79c6>[</span>root@node1 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># ovs-ofctl -O OpenFlow13 show br0</span>
OFPT_FEATURES_REPLY <span style=color:#ff79c6>(</span>OF1.3<span style=color:#ff79c6>)</span> <span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>xid</span><span style=color:#ff79c6>=</span>0x2<span style=color:#ff79c6>)</span>: dpid:0000ea00372f1940
n_tables:254, n_buffers:0
capabilities: FLOW_STATS TABLE_STATS PORT_STATS GROUP_STATS QUEUE_STATS
OFPST_PORT_DESC reply <span style=color:#ff79c6>(</span>OF1.3<span style=color:#ff79c6>)</span> <span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>xid</span><span style=color:#ff79c6>=</span>0x3<span style=color:#ff79c6>)</span>:
 1<span style=color:#ff79c6>(</span>vxlan0<span style=color:#ff79c6>)</span>: addr:72:23:a0:a9:14:a7
     config:     <span style=color:#bd93f9>0</span>
     state:      <span style=color:#bd93f9>0</span>
     speed: <span style=color:#bd93f9>0</span> Mbps now, <span style=color:#bd93f9>0</span> Mbps max
 2<span style=color:#ff79c6>(</span>tun0<span style=color:#ff79c6>)</span>: addr:62:80:67:c6:38:58
     config:     <span style=color:#bd93f9>0</span>
     state:      <span style=color:#bd93f9>0</span>
     speed: <span style=color:#bd93f9>0</span> Mbps now, <span style=color:#bd93f9>0</span> Mbps max
 8381<span style=color:#ff79c6>(</span>vethd040c191<span style=color:#ff79c6>)</span>: addr:7a:d9:f4:12:94:5f
     config:     <span style=color:#bd93f9>0</span>
     state:      <span style=color:#bd93f9>0</span>
     current:    10GB-FD COPPER
     speed: <span style=color:#bd93f9>10000</span> Mbps now, <span style=color:#bd93f9>0</span> Mbps max
 ...
 LOCAL<span style=color:#ff79c6>(</span>br0<span style=color:#ff79c6>)</span>: addr:76:ab:cf:6f:e1:46
     config:     PORT_DOWN
     state:      LINK_DOWN
     speed: <span style=color:#bd93f9>0</span> Mbps now, <span style=color:#bd93f9>0</span> Mbps max
OFPT_GET_CONFIG_REPLY <span style=color:#ff79c6>(</span>OF1.3<span style=color:#ff79c6>)</span> <span style=color:#ff79c6>(</span><span style=color:#8be9fd;font-style:italic>xid</span><span style=color:#ff79c6>=</span>0x5<span style=color:#ff79c6>)</span>: <span style=color:#8be9fd;font-style:italic>frags</span><span style=color:#ff79c6>=</span>nx-match <span style=color:#8be9fd;font-style:italic>miss_send_len</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span>
</code></pre></div><p>考虑到 Openshift 集群的复杂性，我们分别按以下几种场景分析数据包的流向：</p><ul><li>节点内 Pod 互访：Pod to Local Pod</li><li>Pod 跨节点互访：Pod to Remote Pod</li><li>Pod 访问 Service：Pod to Service</li><li>Pod 与集群外部互访：Pod to External</li></ul><p>由于高版本（3.11 以上）的 Openshift 不再以守护进程而是以 Pod 的形式部署 OVS 组件，不方便对 OpenFlow 流表进行查看，因此本文选用的集群版本为 3.6：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#ff79c6>[</span>root@node1 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># oc version </span>
oc v3.6.173.0.5
kubernetes v1.6.1+5115d708d7
features: Basic-Auth GSSAPI Kerberos SPNEGO

Server https://test-cluster.ocp.koktlzz.com:8443
openshift v3.6.173.0.5
kubernetes v1.6.1+5115d708d7
</code></pre></div><p>另外，实验用集群并未开启 ovs-multitenant，即未进行多租户隔离。整个集群 Pod 网络是扁平化的，所有 Pod 的 VNID 都为默认值 0。</p><h2 id=pod-to-local-pod>Pod to Local Pod<a href=#pod-to-local-pod class=anchor aria-hidden=true>#</a></h2><p><img src=https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132046.jpeg alt=202205132046></p><p>数据包首先通过<code>veth-pair</code>送往 OVS 网桥<code>br0</code>，随后便进入了<code>br0</code>上的 OpenFlow 流表。我们可以用 <strong>ovs-ofctl -O OpenFlow13 dump-flows br0</strong> 命令查看流表中的规则，同时为了让输出结果更加简洁，略去 cookie 和 duration 的信息：</p><ul><li><p><code>table=0, n_packets=62751550874, n_bytes=25344802160312, priority=200,ip,in_port=1,nw_src=10.128.0.0/14,nw_dst=10.130.8.0/23 actions=move:NXM_NX_TUN_ID[0..31]->NXM_NX_REG0[],goto_table:10</code></p></li><li><p><code>table=0, n_packets=1081527047094, n_bytes=296066911370148, priority=200,ip,in_port=2 actions=goto_table:30</code></p></li><li><p><code>table=0, n_packets=833353346930, n_bytes=329854403266173, priority=100,ip actions=goto_table:20</code></p><p>table0 中关于 IP 数据包的规则主要有三条，其中前两条分别对应流入端口<code>in_port</code>为 1 号端口<code>vxlan0</code>和 2 号端口<code>tun0</code>的数据包。这两条规则的优先级<code>priority</code>都是 200，因此只有在两者均不符合情况下，才会匹配第三条规则。由于本地 Pod 发出的数据包是由<code>veth</code>端口进入的，因此将转到 table20；</p></li><li><p><code>table=20, n_packets=607178746, n_bytes=218036511085, priority=100,ip,in_port=8422,nw_src=10.130.9.154 actions=load:0->NXM_NX_REG0[],goto_table:21</code></p></li><li><p><code>table=21, n_packets=833757781068, n_bytes=329871389393381, priority=0 actions=goto_table:30</code></p><p>table20 会匹配源地址<code>nw_src</code>为 10.130.9.154 且流入端口<code>in_port</code>为 8422 的数据包，随后将 Pod1 的 VNID 0 作为源 VNID 存入寄存器 0 中，经由 table21 转到 table30；</p></li><li><p><code>table=30, n_packets=1116329752668, n_bytes=294324730186808, priority=200,ip,nw_dst=10.130.8.0/23 actions=goto_table:70</code></p></li><li><p><code>table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90</code></p></li><li><p><code>table=30, n_packets=21061319859, n_bytes=29568807363654, priority=100,ip,nw_dst=172.30.0.0/16 actions=goto_table:60</code></p></li><li><p><code>table=30, n_packets=759636044089, n_bytes=280576476818108, priority=0,ip actions=goto_table:100</code></p><p>table30 中匹配数据包目的地址<code>nw_dst</code>的规则有四条，前三条分别对应本节点内 Pod 的 CIDR 网段 10.130.8.0/23、集群内 Pod 的 CIDR 网段 10.128.0.0/14 和 Service 的 ClusterIP 网段 172.30.0.0/16。第四条优先级最低，用于 Pod 对集群外部的访问。由于数据包的目的地址 10.130.9.158 符合第一条规则，且第一条规则的优先级最高，因此将转到 table70；</p></li><li><p><code>table=70, n_packets=597219981, n_bytes=243824445346, priority=100,ip,nw_dst=10.130.9.158 actions=load:0->NXM_NX_REG1[],load:0x20ea->NXM_NX_REG2[],goto_table:80</code></p><p>table70 匹配目的地址<code>nw_dst</code>为 Pod2 IP 10.130.9.158 的数据包，并将 Pod2 的 VNID 0 作为目的 VNID 存入寄存器 1 中。同时端口号<code>0x20ea</code>被保存到寄存器 2 中，然后转到 table80；</p></li><li><p><code>table=80, n_packets=1112713040332, n_bytes=293801616636499, priority=200 actions=output:NXM_NX_REG2[]</code></p><p>table80 比较寄存器 0 和寄存器 1 中保存的源/目的 VNID。若二者一致，则根据寄存器 2 中保存的端口号将数据包送出。</p></li></ul><p>端口号<code>0x20ea</code>是一个十六进制数字，即十进制数 8426。而 Pod2 正是通过 8426 号端口设备<code>vethba48c6de</code>连接到<code>br0</code>上，因此数据包便最终通过它流入到了 Pod2 中。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#ff79c6>[</span>root@node1 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># ovs-ofctl -O OpenFlow13 show br0 | grep 8426</span>
 8426<span style=color:#ff79c6>(</span>vethba48c6de<span style=color:#ff79c6>)</span>: addr:e6:b2:7e:42:41:91
<span style=color:#ff79c6>[</span>root@node1 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># ip a | grep vethba48c6de</span>
8442: vethba48c6de@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#bd93f9>1450</span> qdisc noqueue master ovs-system state UP 
</code></pre></div><h2 id=pod-to-remote-pod>Pod to Remote Pod<a href=#pod-to-remote-pod class=anchor aria-hidden=true>#</a></h2><p><img src=https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202105132042.jpeg alt=202105132042></p><h3 id=packet-in-local-pod>Packet in Local Pod<a href=#packet-in-local-pod class=anchor aria-hidden=true>#</a></h3><p>数据包依然首先通过<code>veth-pair</code>送往 OVS 网桥<code>br0</code>，随后便进入了<code>br0</code>上的 OpenFlow 流表：</p><ul><li><p><code>table=0, n_packets=830232155588, n_bytes=328613498734351, priority=100,ip actions=goto_table:20</code></p></li><li><p><code>table=20, n_packets=1901, n_bytes=299279, priority=100,ip,in_port=6635,nw_src=10.130.9.154 actions=load:0->NXM_NX_REG0[],goto_table:21</code></p></li><li><p><code>table=21, n_packets=834180030914, n_bytes=330064497351030, priority=0 actions=goto_table:30</code></p><p>与 Pod to Local Pod 的流程一致，数据包根据规则转到 table30；</p></li><li><p><code>table=30, n_packets=59672345347, n_bytes=41990349575805, priority=100,ip,nw_dst=10.128.0.0/14 actions=goto_table:90</code></p></li><li><p><code>table=30, n_packets=1116329752668, n_bytes=294324730186808, priority=200,ip,nw_dst=10.130.8.0/23 actions=goto_table:70</code></p><p>数据包的目的地址为 Pod2 IP 10.131.8.206，不属于本节点 Pod 的 CIDR 网段 10.130.8.0/23，而属于集群 Pod 的 CIDR 网段 10.128.0.0/14，因此转到 table90；</p></li><li><p><code>table=90, n_packets=15802525677, n_bytes=6091612778189, priority=100,ip,nw_dst=10.131.8.0/23 actions=move:NXM_NX_REG0[]->NXM_NX_TUN_ID[0..31],set_field:10.122.28.8->tun_dst,output:1</code></p><p>table90 根据目的 IP 的所属网段 10.131.8.0/23 判断其位于 Node2 上，于是将 Node2 IP 10.122.28.8 设置为<code>tun_dst</code>。并且从寄存器 0 中取出 VNID 的值，从 1 号端口<code>vxlan0</code>输出。</p></li></ul><p><code>vxlan0</code>作为一个 VTEP 设备（参见 [Overlay Network](/kubernetes/kubernetes 进阶/sdn/#overlay-network)），将根据 table90 发来的信息，对数据包进行一层封装：</p><ul><li>目的地址（dst IP） &ndash;> <code>tun_dst</code> &ndash;> 10.122.28.8</li><li>源地址（src IP） &ndash;> Node1 IP &ndash;> 10.122.28.7</li><li>源 VNID &ndash;> <code>NXM_NX_TUN_ID[0..31]</code> &ndash;> 0</li></ul><p>由于封装后的数据包源/目的地址均为节点 IP，因此从 Node1 的网卡流出后，可以通过物理网络设备转发到 Node2 上。</p><h3 id=packet-in-remote-pod>Packet in Remote Pod<a href=#packet-in-remote-pod class=anchor aria-hidden=true>#</a></h3><p>Node2 上的<code>vxlan0</code>对数据包进行解封，随后从<code>br0</code>上的 1 号端口进入 OpenFlow 流表中：</p><ul><li><p><code>table=0, n_packets=52141153195, n_bytes=17269645342781, priority=200,ip,in_port=1,nw_src=10.128.0.0/14,nw_dst=10.131.8.0/23 actions=move:NXM_NX_TUN_ID[0..31]->NXM_NX_REG0[],goto_table:10</code></p><p>table0 判断数据包的流入端口<code>in_port</code>、源 IP 所属网段<code>nw_src</code>和目的 IP 所属网段<code>nw_dst</code>均符合该条规则，于是保存数据包中的源 VNID 到寄存器 0 后转到 table10；</p></li><li><p><code>table=10, n_packets=10147760036, n_bytes=4060517391502, priority=100,tun_src=10.122.28.7 actions=goto_table:30</code></p><p>table10 确认 VxLAN 隧道的源 IP<code>tun_src</code>就是节点 Node1 的 IP 地址，于是转到 table30；</p></li><li><p><code>table=30, n_packets=678759566065, n_bytes=172831151192704, priority=200,ip,nw_dst=10.131.8.0/23 actions=goto_table:70</code></p><p>table30 确认数据包的目的 IP（即 Pod2 IP）存在于 Node2 中 Pod 的 CIDR 网段内，因此转到 table70；</p></li><li><p><code>table=70, n_packets=193211683, n_bytes=27881218388, priority=100,ip,nw_dst=10.131.8.206 actions=load:0->NXM_NX_REG1[],load:0x220->NXM_NX_REG2[],goto_table:80</code></p><p>table70 发现数据包的目的 IP 与 Pod2 IP 相符，于是将 Pod2 的 VNID 作为目的 VNID 存于寄存器 1 中，将<code>0x220</code>（十进制数 544）保存在寄存器 2 中，然后转到 table80；</p></li><li><p><code>table=80, n_packets=676813794014, n_bytes=172576112594488, priority=200 actions=output:NXM_NX_REG2[]</code></p><p>table80 会检查保存在寄存器 0 和寄存器 1 中的源/目的 VNID，若相等（此例中均为 0），则从 544 号端口输出。</p></li></ul><p><code>br0</code>上的 554 端口对应的网络接口是<code>vethe9f523a9</code>，因此数据包便最终通过它流入到了 Pod2 中。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#ff79c6>[</span>root@node2 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># ovs-ofctl -O OpenFlow13 show br0 | grep 544</span>
 544<span style=color:#ff79c6>(</span>vethe9f523a9<span style=color:#ff79c6>)</span>: addr:b2:a1:61:00:dc:3b
<span style=color:#ff79c6>[</span>root@node2 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># ip a show vethe9f523a9</span>
559: vethe9f523a9@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#bd93f9>1450</span> qdisc noqueue master ovs-system state UP 
    link/ether b2:a1:61:00:dc:3b brd ff:ff:ff:ff:ff:ff link-netnsid <span style=color:#bd93f9>54</span>
    inet6 fe80::b0a1:61ff:fe00:dc3b/64 scope link 
       valid_lft forever preferred_lft forever
</code></pre></div><h2 id=pod-to-service>Pod to Service<a href=#pod-to-service class=anchor aria-hidden=true>#</a></h2><p>在本例中，Pod1 通过 Service 访问其后端的 Pod2，其 ClusterIP 为 172.30.107.57，监听的端口为 8080：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#ff79c6>[</span>root@node1 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># oc get svc</span>
NAME             CLUSTER-IP      EXTERNAL-IP   PORT<span style=color:#ff79c6>(</span>S<span style=color:#ff79c6>)</span>    AGE
myService        172.30.107.57   &lt;none&gt;        8080/TCP   2y
</code></pre></div><ul><li><p><code>table=30, n_packets=21065939280, n_bytes=29573447694924, priority=100,ip,nw_dst=172.30.0.0/16 actions=goto_table:60</code></p><p>数据包在送到 OpenFlow 流表 table30 前的步骤与 Pod to Local Pod 和 Pod to Remote Pod 中的情况一致，但数据包的目的地址变为了 myService 的 ClusterIP。因此将匹配<code>nw_dst</code>中的 172.30.0.0/16 网段，转到 table60；</p></li><li><p><code>table=60, n_packets=0, n_bytes=0, priority=100,tcp,nw_dst=172.30.107.57,tp_dst=8080 actions=load:0->NXM_NX_REG1[],load:0x2->NXM_NX_REG2[],goto_table:80</code></p><p>table60 匹配目的地址<code>nw_dst</code>为 172.30.107.57 且目的端口为 8080 的数据包，并将 Pod1 的 VNID 0 保存到寄存器 1 中，将<code>0x2</code>（十进制数字 2）保存到寄存器 2 中，转到 table80；</p></li><li><p><code>table=80, n_packets=1113435014018, n_bytes=294106102133061, priority=200 actions=output:NXM_NX_REG2[]</code></p><p>table80 首先检查目的 Service 的 VNID 是否与寄存器 1 中的 VNID 一致，然后根据寄存器 2 中的数字将数据包从 2 号端口<code>tun0</code>送出，最后进入节点的 iptables 规则中。</p></li></ul><p>iptables 对数据包的处理流程如下图所示：</p><p><img src=https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/20210516142844.png alt=20210516142844></p><p>由于 Service 的实现依赖于 NAT（上图中的紫色方框），因此我们可以在 NAT 表中查看到与之相关的规则：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#ff79c6>[</span>root@node1 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># iptables -t nat -nvL</span>
Chain OUTPUT <span style=color:#ff79c6>(</span>policy ACCEPT <span style=color:#bd93f9>4753</span> packets, 489K bytes<span style=color:#ff79c6>)</span>
 pkts bytes target     prot opt in     out     <span style=color:#8be9fd;font-style:italic>source</span>               destination         
2702M  274G KUBE-SERVICES  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* kubernetes service portals */

Chain KUBE-SERVICES <span style=color:#ff79c6>(</span><span style=color:#bd93f9>2</span> references<span style=color:#ff79c6>)</span>
 pkts bytes target     prot opt in     out     <span style=color:#8be9fd;font-style:italic>source</span>               destination
    <span style=color:#bd93f9>4</span>   <span style=color:#bd93f9>240</span> KUBE-SVC-QYWOVDCBPMWAGC37  tcp  --  *      *       0.0.0.0/0            172.30.107.57        /* demo/myService:8080-8080 cluster IP */ tcp dpt:8080
</code></pre></div><p>本机产生的数据包（Locally-generated Packet）首先进入<code>OUTPUT</code>链，然后匹配到自定义链<code>KUBE-SERVICES</code>。由于其目的地址为 Service 的 ClusterIP 172.30.107.57，因此将再次跳转到对应的<code>KUBE-SVC-QYWOVDCBPMWAGC37</code>链：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>Chain KUBE-SVC-QYWOVDCBPMWAGC37 <span style=color:#ff79c6>(</span><span style=color:#bd93f9>1</span> references<span style=color:#ff79c6>)</span>
 pkts bytes target     prot opt in     out     <span style=color:#8be9fd;font-style:italic>source</span>               destination         
    <span style=color:#bd93f9>1</span>    <span style=color:#bd93f9>60</span> KUBE-SEP-AF5DIL6JV3XLLV6G  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* demo/myService:8080-8080 */ statistic mode random probability 0.50000000000
    <span style=color:#bd93f9>1</span>    <span style=color:#bd93f9>60</span> KUBE-SEP-ADAJHSV7RYS5DUBX  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* demo/myService:8080-8080 */

Chain KUBE-SEP-ADAJHSV7RYS5DUBX <span style=color:#ff79c6>(</span><span style=color:#bd93f9>1</span> references<span style=color:#ff79c6>)</span>
 pkts bytes target     prot opt in     out     <span style=color:#8be9fd;font-style:italic>source</span>               destination         
    <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span> KUBE-MARK-MASQ  all  --  *      *       10.131.8.206         0.0.0.0/0            /* demo/myService:8080-8080 */
    <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span> DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* demo/myService:8080-8080 */ tcp to:10.131.8.206:8080

Chain KUBE-SEP-AF5DIL6JV3XLLV6G <span style=color:#ff79c6>(</span><span style=color:#bd93f9>1</span> references<span style=color:#ff79c6>)</span>
 pkts bytes target     prot opt in     out     <span style=color:#8be9fd;font-style:italic>source</span>               destination         
    <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span> KUBE-MARK-MASQ  all  --  *      *       10.128.10.57         0.0.0.0/0            /* demo/myService:8080-8080 */
   <span style=color:#bd93f9>23</span>  <span style=color:#bd93f9>1380</span> DNAT       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0            /* demo/myService:8080-8080 */ tcp to:10.128.10.57:8080
</code></pre></div><p><code>KUBE-SVC-QYWOVDCBPMWAGC37</code>链下有两条完全相同的匹配规则，对应了该 Service 后端的两个 Pod。<code>KUBE-SEP-ADAJHSV7RYS5DUBX</code>链和 <code>KUBE-SEP-AF5DIL6JV3XLLV6G</code>链能够执行 DNAT 操作，分别将数据包的目的地址转化为 Pod IP 10.131.8.206 和 10.128.10.57。在一次通信中只会有一条链生效，这体现了 Service 的负载均衡能力。</p><p>完成<code>OUTPUT</code>DNAT 的数据包将进入节点的路由判断（Routing Decision）。由于当前目的地址已经属于集群内 Pod 的 CIDR 网段 10.128.0.0/14，因此将再次从<code>tun0</code>端口再次进入 OVS 网桥<code>br0</code>中。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#ff79c6>[</span>rootnode1 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># route -n</span>
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         10.122.28.1     0.0.0.0         UG    <span style=color:#bd93f9>0</span>      <span style=color:#bd93f9>0</span>        <span style=color:#bd93f9>0</span> eth0
10.122.28.0     0.0.0.0         255.255.255.128 U     <span style=color:#bd93f9>0</span>      <span style=color:#bd93f9>0</span>        <span style=color:#bd93f9>0</span> eth0
10.128.0.0      0.0.0.0         255.252.0.0     U     <span style=color:#bd93f9>0</span>      <span style=color:#bd93f9>0</span>        <span style=color:#bd93f9>0</span> tun0
169.254.0.0     0.0.0.0         255.255.0.0     U     <span style=color:#bd93f9>1008</span>   <span style=color:#bd93f9>0</span>        <span style=color:#bd93f9>0</span> eth0
172.17.0.0      0.0.0.0         255.255.0.0     U     <span style=color:#bd93f9>0</span>      <span style=color:#bd93f9>0</span>        <span style=color:#bd93f9>0</span> docker0
172.30.0.0      0.0.0.0         255.255.0.0     U     <span style=color:#bd93f9>0</span>      <span style=color:#bd93f9>0</span>        <span style=color:#bd93f9>0</span> tun0
</code></pre></div><p>不过数据包在进入<code>br0</code>之前，还需要经过 iptables 中的<code>POSTROUTING</code>链，完成一次 MASQUERADE 操作：数据包的源地址转换为其流出端口的 IP，即<code>tun0</code>的 IP 10.130.8.1。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#ff79c6>[</span>root@node1 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># iptables -t nat -nvL </span>
Chain POSTROUTING <span style=color:#ff79c6>(</span>policy ACCEPT <span style=color:#bd93f9>5083</span> packets, 524K bytes<span style=color:#ff79c6>)</span>
 pkts bytes target     prot opt in     out     <span style=color:#8be9fd;font-style:italic>source</span>               destination                
2925M  288G OPENSHIFT-MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* rules <span style=color:#ff79c6>for</span> masquerading OpenShift traffic */

Chain OPENSHIFT-MASQUERADE <span style=color:#ff79c6>(</span><span style=color:#bd93f9>1</span> references<span style=color:#ff79c6>)</span>
 pkts bytes target     prot opt in     out     <span style=color:#8be9fd;font-style:italic>source</span>               destination         
 321M   19G MASQUERADE  all  --  *      *       10.128.0.0/14        0.0.0.0/0            /* masquerade pod-to-service and pod-to-external traffic */
<span style=color:#ff79c6>[</span>root@node1 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># ip a | grep tun0</span>
16: tun0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style=color:#bd93f9>1450</span> qdisc noqueue state UNKNOWN qlen <span style=color:#bd93f9>1000</span>
    inet 10.130.8.1/23 scope global tun0
</code></pre></div><p>本例中 Service 的后端 Pod 均在 Pod1 所在的节点外，因此数据包第二次进入 OpenFlow 流表时匹配的规则与 Pod to Remote Pod 一致。其传递流程如下图所示：</p><p><img src=https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132044.jpeg alt=202205132044></p><p>Pod2 返回的数据包在到达 Node1 后将被<code>vxlan0</code>解封装，然后根据其目的地址<code>tun0</code>进入 OpenFlow 流表：</p><ul><li><code>table=0, n_packets=1084362760247, n_bytes=297224518823222, priority=200,ip,in_port=2 actions=goto_table:30</code></li><li><code>table=30, n_packets=20784385211, n_bytes=4742514750371, priority=300,ip,nw_dst=10.130.8.1 actions=output:2</code></li></ul><p>数据包从 2 号端口<code>tun0</code>流出后进入节点的 iptables 规则，随后将触发 iptables 的 <a href=https://superuser.com/questions/1269859/linux-netfilter-how-does-connection-tracking-track-connections-changed-by-nat>Connection Tracking</a> 操作：根据 <strong>/proc/net/nf_conntrack</strong> 文件中的记录进行“DeNAT”。返回数据包的源/目的地址从 Pod2 IP 10.131.8.206 和 tun0 IP 10.130.8.1，变回 Service 的 ClusterIP 172.30.107.57 和 Pod1 IP 10.130.9.154。</p><div class=highlight><pre style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=color:#ff79c6>[</span>root@node1 ~<span style=color:#ff79c6>]</span><span style=color:#6272a4># cat /proc/net/nf_conntrack | grep -E &#34;src=10.130.9.154.*dst=172.30.107.57.*dport=8080.*src=10.131.8.206&#34;</span>
ipv4     <span style=color:#bd93f9>2</span> tcp      <span style=color:#bd93f9>6</span> <span style=color:#bd93f9>431986</span> ESTABLISHED <span style=color:#8be9fd;font-style:italic>src</span><span style=color:#ff79c6>=</span>10.130.9.154 <span style=color:#8be9fd;font-style:italic>dst</span><span style=color:#ff79c6>=</span>172.30.107.57 <span style=color:#8be9fd;font-style:italic>sport</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>80</span> <span style=color:#8be9fd;font-style:italic>dport</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>8080</span> <span style=color:#8be9fd;font-style:italic>src</span><span style=color:#ff79c6>=</span>10.131.8.206 <span style=color:#8be9fd;font-style:italic>dst</span><span style=color:#ff79c6>=</span>10.130.8.1 <span style=color:#8be9fd;font-style:italic>sport</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>8080</span> <span style=color:#8be9fd;font-style:italic>dport</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>80</span> <span style=color:#ff79c6>[</span>ASSURED<span style=color:#ff79c6>]</span> <span style=color:#8be9fd;font-style:italic>mark</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span> <span style=color:#8be9fd;font-style:italic>secctx</span><span style=color:#ff79c6>=</span>system_u:object_r:unlabeled_t:s0 <span style=color:#8be9fd;font-style:italic>zone</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>0</span> <span style=color:#8be9fd;font-style:italic>use</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>2</span>
</code></pre></div><h2 id=pod-to-external>Pod to External<a href=#pod-to-external class=anchor aria-hidden=true>#</a></h2><p>数据包依然首先通过<code>veth-pair</code>送往 OVS 网桥<code>br0</code>，随后便进入了<code>br0</code>上的 OpenFlow 流表：</p><ul><li><code>table=0, n_packets=837268653828, n_bytes=331648403594327, priority=100,ip actions=goto_table:20</code></li><li><code>table=20, n_packets=613807687, n_bytes=220557571042, priority=100,ip,in_port=8422,nw_src=10.130.9.154 actions=load:0->NXM_NX_REG0[],goto_table:21</code></li><li><code>table=21, n_packets=837674296060, n_bytes=331665441915651, priority=0 actions=goto_table:30</code></li><li><code>table=30, n_packets=759636044089, n_bytes=280576476818108, priority=0,ip actions=goto_table:100</code></li><li><code>table=100, n_packets=761732023982, n_bytes=282091648536325, priority=0 actions=output:2</code></li></ul><p>数据包从<code>tun0</code>端口流出后进入节点的路由表及 iptables 规则：</p><div class=highlight><pre style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>Chain POSTROUTING <span style=color:#ff79c6>(</span>policy ACCEPT <span style=color:#bd93f9>2910</span> packets, 299K bytes<span style=color:#ff79c6>)</span>
 pkts bytes target     prot opt in     out     <span style=color:#8be9fd;font-style:italic>source</span>               destination         
    <span style=color:#bd93f9>0</span>     <span style=color:#bd93f9>0</span> MASQUERADE  all  --  *      !docker0  172.17.0.0/16        0.0.0.0/0           
2940M  289G OPENSHIFT-MASQUERADE  all  --  *      *       0.0.0.0/0            0.0.0.0/0            /* rules <span style=color:#ff79c6>for</span> masquerading OpenShift traffic */

Chain OPENSHIFT-MASQUERADE <span style=color:#ff79c6>(</span><span style=color:#bd93f9>1</span> references<span style=color:#ff79c6>)</span>
 pkts bytes target     prot opt in     out     <span style=color:#8be9fd;font-style:italic>source</span>               destination         
 322M   19G MASQUERADE  all  --  *      *       10.128.0.0/14        0.0.0.0/0            /* masquerade pod-to-service and pod-to-external traffic */
</code></pre></div><p>访问集群外部显然需要通过节点的默认网关，因此数据包将从节点网卡<code>eth0</code>送出。而在<code>POSTROUTING</code>链中，数据包的源地址由 Pod IP 转换为了<code>eth0</code>的 IP 10.122.28.7。完整流程如下图所示：</p><p><img src=https://cdn.jsdelivr.net/gh/koktlzz/ImgBed@master/202205132045.jpeg alt=202205132045></p><h2 id=参考文献>参考文献<a href=#参考文献 class=anchor aria-hidden=true>#</a></h2><p><a href=https://medoc.readthedocs.io/en/latest/docs/ovs/sharing/cloud_usage.html>OVS 在云项目中的使用</a></p><p><a href=https://docs.openshift.com/container-platform/3.11/architecture/networking/sdn.html>OpenShift SDN - Networking | Architecture | OpenShift Container Platform 3.11</a></p><p><a href=https://www.cnblogs.com/sammyliu/p/10064450.html>理解 OpenShift（3）：网络之 SDN</a></p><p class=edit-page><a href=https://github.com/koktlzz/koktlzz.github.io/blob/master/content/Kubernetes/Kubernetes%e8%bf%9b%e9%98%b6/OpenvSwitch.md><svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-edit-2"><path d="M17 3a2.828 2.828.0 114 4L7.5 20.5 2 22l1.5-5.5L17 3z"/></svg>Edit this page on GitHub</a></p><div class="docs-navigation d-flex justify-content-between"><a href=https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/sdn/><div class="card my-1"><div class="card-body py-2">&larr; SDN</div></div></a></div><script src=https://utteranc.es/client.js repo=koktlzz/koktlzz.github.io issue-term=pathname theme=photon-dark crossorigin=anonymous async></script></main><nav class="docs-toc d-none d-xl-block col-lg-3 col-xl-2" aria-label="Secondary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#前言>前言</a></li><li><a href=#pod-to-local-pod>Pod to Local Pod</a></li><li><a href=#pod-to-remote-pod>Pod to Remote Pod</a><ul><li><a href=#packet-in-local-pod>Packet in Local Pod</a></li><li><a href=#packet-in-remote-pod>Packet in Remote Pod</a></li></ul></li><li><a href=#pod-to-service>Pod to Service</a></li><li><a href=#pod-to-external>Pod to External</a></li><li><a href=#参考文献>参考文献</a></li></ul></nav></div></nav></div></div></div><footer class="footer text-muted"><div class=container><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline></ul></div></div></div></footer><script src=https://koktlzz.github.io/js/bootstrap.min.d67050adf5d370668aede4201f82af781b16970934804995a1ca37c1ee9222c2fc530972aa6d5d2b6124caf1fe318f139aac99df2c1e89af65504fc1185c7972.js integrity="sha512-1nBQrfXTcGaK7eQgH4KveBsWlwk0gEmVoco3we6SIsL8Uwlyqm1dK2EkyvH+MY8TmqyZ3yweia9lUE/BGFx5cg==" crossorigin=anonymous defer></script><script src=https://koktlzz.github.io/js/vendor/katex/dist/katex.min.b334e5b00c45f572e7f38a543046f36854bf4cbb543880480c09271a0efedc694d6905f719934f6ede2f26f52e80411c05b90121d722cbf8be2fa8fc51bc6b81.js integrity="sha512-szTlsAxF9XLn84pUMEbzaFS/TLtUOIBIDAknGg7+3GlNaQX3GZNPbt4vJvUugEEcBbkBIdciy/i+L6j8UbxrgQ==" crossorigin=anonymous defer></script><script src=https://koktlzz.github.io/js/vendor/katex/dist/contrib/auto-render.min.916823ec103cf367b71e28a6f01513cb9c3ac6708ccb5229402ec46b8e30a8b25003db694df35d80e9dd666a371f327c6152790617b6256390c164109a90bd4c.js integrity="sha512-kWgj7BA882e3Hiim8BUTy5w6xnCMy1IpQC7Ea44wqLJQA9tpTfNdgOndZmo3HzJ8YVJ5Bhe2JWOQwWQQmpC9TA==" crossorigin=anonymous defer></script><script src=https://koktlzz.github.io/main.min.f6e8faaea2ce6bb257f34da94268a7e539d1da8094519ef5a226be2646792c13b4b992b9c31c3f9ce3d3e4a39acbd7e88d9a46666014e9371baa755131320bca.js integrity="sha512-9uj6rqLOa7JX802pQmin5TnR2oCUUZ71oia+JkZ5LBO0uZK5wxw/nOPT5KOay9fojZpGZmAU6TcbqnVRMTILyg==" crossorigin=anonymous defer></script><script src=https://koktlzz.github.io/index.min.bc9d0cdd7fc7b444e89eeabd4981596119f6edd9f1d94c19eb112cbe44a272a177dffa9c9820a54e7eca47e67e3d3377a9a030c0569085f92565b915fcd317a0.js integrity="sha512-vJ0M3X/HtETonuq9SYFZYRn27dnx2UwZ6xEsvkSicqF33/qcmCClTn7KR+Z+PTN3qaAwwFaQhfklZbkV/NMXoA==" crossorigin=anonymous defer></script></body></html>