<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes on</title><link>https://koktlzz.github.io/kubernetes/</link><description>Recent content in Kubernetes on</description><generator>Hugo -- gohugo.io</generator><language>en-US</language><lastBuildDate>Wed, 04 Nov 2020 09:19:42 +0100</lastBuildDate><atom:link href="https://koktlzz.github.io/kubernetes/index.xml" rel="self" type="application/rss+xml"/><item><title>Getting Started</title><link>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/intro/</link><pubDate>Wed, 04 Nov 2020 09:19:42 +0100</pubDate><guid>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/intro/</guid><description>官方网站 https://kubernetes.io/
推荐阅读 官方文档
Kubernetes 教程｜Kuboard
Kubernetes Overview Diagrams
Kubernetes - 面向信仰编程</description></item><item><title>Infrastructure</title><link>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/infrastructure/</link><pubDate>Wed, 04 Nov 2020 09:19:42 +0100</pubDate><guid>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/infrastructure/</guid><description>架构 基本特性 声明式：Kubernetes 中常用 yaml 文件定义服务和资源的拓扑结构和状态，它是一种声明式的编程方式，更加关注状态和结果。其实我们最常接触的是命令式编程，它要求我们描述为了达到某一个效果或者目标所需要完成的指令。常见的编程语言 Go、Ruby、C++ 以及我们使用的 kubectl 工具其实都属于命令式的编程方法； 显式接口：不存在内部的私有接口； 无侵入：每一个应用或者服务一旦被打包成了镜像就可以直接在 Kubernetes 中无缝使用，不需要修改应用程序中的任何代码； 可移植：支持有状态服务的迁移和持久化存储。 组件 Master 组件 etcd：保存了整个集群的状态； Api-server：负责处理来自用户的请求，其主要作用就是对外提供 RESTful 的接口。包括用于查看集群状态的读请求以及改变集群状态的写请求，也是唯一一个与 etcd 集群通信的组件； Controller-manager：运行了一系列的 Controller 进程，它们会按照用户的期望状态在后台不断地调节整个集群中的对象。当服务的状态发生了改变，控制器就会发现这个改变并且开始向目标状态迁移。每种资源都有其对应的 Controller； Scheduler：负责资源的调度，采用预算策略或优选策略将 Pod 调度到合适的节点上。 Node 组件 Kubelet：负责维持容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理。它周期性地从 API Server 获取 Pod 的 Spec，并确保容器处于运行状态且保持健康； Kube-proxy：负责为 Service 提供集群内部的服务发现和负载均衡； Container Runtime：负责运行容器的软件，如 Docker、CRI-O 以及所有实现 Kubernetes CRI（Container Runtime Interface）的应用。 Client 组件 kubeadm：部署工具，提供 kubeadm init 和 kubeadm join，用于快速部署 Kubernetes 集群； kubectl：用户或开发者使用的命令行工具。 插件 Addons CoreDNS：负责为整个集群提供 DNS 服务； Ingress Controller：为 Service 提供从集群外部访问的入口； Heapster：提供资源监控； Dashboard：提供 GUI； Federation：提供跨可用区的集群； Fluentd/Elasticsearch：提供集群日志采集、存储与查询； Prometheus：提供集群的监控能力。 资源与对象 Kubernetes 中的所有内容都被抽象为“资源”，如 Pod、Service、Node 等都是资源。“对象”就是“资源”的实例，是持久化的实体。如某个具体的 Pod、某个具体的 Node。Kubernetes 使用这些实体去表示整个集群的状态。 对象的创建、删除、修改都是通过 Kubernetes API，也就是 Api Server 组件提供的 API 接口，这些是 RESTful 风格的 Api，与 k8s 的万物皆对象理念相符。命令行工具 kubectl，实际上也是调用 kubernetes api。 Kubernetes REST API 中的所有对象都用 Name 和 UID 来明确地标识。</description></item><item><title>Pod 与 Namespace</title><link>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/pod%E4%B8%8Enamespace/</link><pubDate>Wed, 04 Nov 2020 09:19:42 +0100</pubDate><guid>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/pod%E4%B8%8Enamespace/</guid><description>Pod 基本特性 Pod 是 Kubernetes 中的最小单位，由一个或一组容器组成。一个容器中运行一个进程，而一个 Pod 运行多个进程； 为实现亲密性应用，Pod 内部的容器共享网络和挂载卷，因此可以方便地实现多个应用间交互； 每一个 Pod 其实都具有两种不同的容器，一种是 Init 容器：在 Pod 启动时运行，主要用于初始化一些配置。另一种是 Pod 在 Running 状态时内部存活的应用容器，它们的主要作用是对外提供服务或者作为工作节点处理异步任务等； Pod 的生命周期短暂。 Spec 和 Status 以一个名为 busybox 的 Pod 为例：
apiVersion: v1 kind: Pod metadata: name: busybox labels: app: busybox spec: containers: - image: busybox command: - sleep - &amp;#34;3600&amp;#34; resources: requests: cpu: &amp;#34;500m&amp;#34; memory: &amp;#34;512Mi&amp;#34; limits: cpu: &amp;#34;500m&amp;#34; memory: &amp;#34;512Mi&amp;#34; imagePullPolicy: IfNotPresent name: busybox restartPolicy: Always Pod 的 Spec 指定了 Pod 中包含的容器以及容器的镜像、启动命令等信息，上述例子中还包括了镜像拉取策略、容器资源限制以及 Pod 重启策略。 而每一个 Pod 的 Status 包含了生命周期、当前服务状态、宿主机和 Pod 的 IP 地址以及其中内部所有容器的状态信息等：</description></item><item><title>Service</title><link>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/service/</link><pubDate>Wed, 04 Nov 2020 09:19:42 +0100</pubDate><guid>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/service/</guid><description>概述 Kubernetes 使用 Service 解决服务发现问题：每个 Pod 在创建后都会被分配一个 IP 地址，然而它会随着 Pod 的重启而改变； Service 可以通过标签选择器选择一组 Pod，然后作为它们共同的对外访问接口。这样我们的应用便可以在不知道 Pod 的 IP 地址的情况下，与其通信； 当 Service 的标签选择器选择了多个 Pod 时，还可以在它们之间做负载均衡； 众所周知，Service 的中文意为“服务”。但就其功能而言，更像是一个 Proxy（代理）或 Router（路由）。 配置 一个典型的 Service 对象配置如下：
kind: Service apiVersion: v1 metadata: name: nginx-server spec: clusterIP: 192.168.1.0 selector: app: nginx ports: - name: http protocol: TCP port: 80 targetPort: 9376 - name: https protocol: TCP port: 443 targetPort: 9377 每个 Service 都会由系统分配一个虚拟 IP 作为访问 Service 的入口 IP 地址，然后监听spec.</description></item><item><title>Volume</title><link>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/volume/</link><pubDate>Wed, 04 Nov 2020 09:19:42 +0100</pubDate><guid>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/volume/</guid><description>意义 Kubernetes 引入 Volume 资源来解决以下问题：
容器中的文件在磁盘上是临时存放的，kubelet 重启容器后，文件将会丢失； 在运行多个容器的 Pod 内实现文件共享。 配置 一个典型的有挂载卷的 Pod 配置如下：
apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - image: docker.io/nginx:latest name: nginx volumeMounts: - mountPath: /nginx-master-data name: test-volume volumes: - name: test-volume emptyDir: {} spec.containers.volumeMounts.mountPath字段定义了 Volume 在容器中的挂载位置； 字段spec.containers.volumeMounts与spec.volumes.name必须相同； spec.volumes.emptyDir代表 Volume 的类型。 类型 Volume 的核心是包含一些数据的一个目录，Pod 中的容器可以访问该目录。Volume 的类型将决定该目录如何形成的、使用何种介质保存数据以及目录中存放的内容。
emptyDir 在当前 Pod 对应的目录创建了一个空的文件夹，这个文件夹会随着 Pod 的删除而删除。
hostPath 将 Pod 所在节点文件系统上的文件或目录挂载到 Pod 中。
nfs 将 NFS（Network File System）挂载到 Pod 中，可以在不同节点上的不同 Pod 之间共享数据并被多个 Pod 同时读写。当 Pod 被移除时，将仅仅卸载 nfs 数据卷，Volume 中的数据仍将被保留；</description></item><item><title>Network Policy</title><link>https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/networkpolicy/</link><pubDate>Wed, 04 Nov 2020 09:19:42 +0100</pubDate><guid>https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/networkpolicy/</guid><description>概述 如果希望在 IP 地址或端口层面（OSI 第 3 层或第 4 层）控制网络流量，那么可以为集群中的特定应用使用 Kubernetes 网络策略（NetworkPolicy）。能够与 Pod 通信的对象是通过以下三个标识符的组合来规定的：
其他被允许的 Pods（Pod 无法阻塞对自身的访问） 被允许的 Namespace IP/port（与 Pod 运行所在的节点的通信总是被允许的，无论是 Pod 还是节点的 IP 地址） 当同一个 Pod 被多个 Network Policy 通过标签选择器选择时，其网络策略为多个 Network Policy 的并集。
配置 apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: test-network-policy namespace: default # namespace spec: podSelector: # 若为空，选择 default-namespace 下的所有 pod matchLabels: role: db policyTypes: - Ingress # 入站策略 - Egress # 出站策略 ingress: - from: - ipBlock: # 白名单 ip 网段 cidr: 172.</description></item><item><title>Workloads</title><link>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/workloads/</link><pubDate>Wed, 04 Nov 2020 09:19:42 +0100</pubDate><guid>https://koktlzz.github.io/kubernetes/kubernetes%E5%9F%BA%E7%A1%80/workloads/</guid><description>简介 Workloads（工作负载）资源是 Kubernetes 上运行的应用程序，可以用来创建和管理多个 Pod，提供副本管理、健康检查、滚动升级和集群级别的自愈能力。例如，如果一个节点故障，Workloads 配置的 Controller 就能自动将该节点上的 Pod 调度到其他健康的节点上。Workloads Controller 运行在 Kubernetes 集群的主节点上，它们不断控制集群中的资源向期望状态迁移（stauts -&amp;gt; spec）。常用的 Workloads 资源有：
ReplicaSet 决定一个 Pod 有多少同时运行的副本，并保证这些副本的期望状态与当前状态一致。
配置 一个典型的 ReplicaSet 配置如下：
apiVersion: apps/v1 kind: ReplicaSet metadata: name: frontend labels: app: guestbook tier: frontend spec: replicas: 3 # 副本数 selector: matchLabels: tier: frontend template: metadata: labels: tier: frontend spec: containers: - name: php-redis image: docker.io/redis:latest 在上述配置信息中，字段spec.template.metadata.labels的值必须与spec.selector值相匹配，否则创建请求会被 Kubernetes API 拒绝； 被 ReplicaSet 控制的 Pod 在创建或更新后，其metadata.ownerReferences字段会添加该 ReplicaSet 的信息； 一旦删除了原来的 ReplicaSet，就可以创建一个新的来替换它。只要新旧 ReplicaSet 的spec.</description></item><item><title>Ingress</title><link>https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/ingress/</link><pubDate>Wed, 04 Nov 2020 09:19:42 +0100</pubDate><guid>https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/ingress/</guid><description>概述 在集群外部，我们可以通过 NodePort 类型的 Service 或 LoadBalancer 类型的 Service 来访问集群内部的 Pod。由于它们的实现均基于 IP 地址和端口并通常使用 TCP 协议，因此属于四层网络模型。
Ingress 则基于七层网络模型，可以向集群外部提供可访问的 URL 并将发往 Ingress 负载均衡器的外部 HTTP 或 HTTPS 请求转发到集群内部的 Service。一个将所有流量都发送到同一 Service 下 Pod 的简单 Ingress 示意图如下：
如图所示，集群外的客户端向 Ingress 负载均衡器发送 HTTP(s) 请求，流量将被转发到节点的 80(443) 端口上。同时，集群内部的 Ingress 对象监听节点的 80(443) 端口，因此流量将被其接收。Ingress 对象将请求的 URL 和目标主机名与其配置的规则进行匹配，并将流量转发到对应的 Service。最后由 Service 将流量转发到后端的 Pod。
在 Openshift 中，提供 Ingress 功能的组件被称为 Router。它以 Pod 的形式部署在集群的 Infra 或 Router 节点上，通过 hostnetwork 对外暴露 80（443）端口，其内部则运行着 HAProxy 服务。Router 会从 etcd 中查询 Route 对象绑定的 Service 的后端 Pod Endpoint 信息，并将其写入到 HAProxy 的路由规则中。这样当 Router 接收到发往其所在节点 80（443）端口的流量时，HAProxy 就可以直接将其转发到对应的 Pod 而无需经过 Service。</description></item><item><title>SDN</title><link>https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/sdn/</link><pubDate>Wed, 04 Nov 2020 09:19:42 +0100</pubDate><guid>https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/sdn/</guid><description>前言 A Guide to the Kubernetes Networking Model 一文生动形象地介绍了 Kubernetes 中的网络模型，然而受篇幅所限，作者并没有对 Pod 跨节点通信时数据包在节点之间传递的细节进行过多讨论。
我们已经知道，Docker 使用端口映射的方式实现不同主机间容器的通信，Kubernetes 中同样也有 hostPort 的概念。但是当节点和 Pod 的数量上升后，手动管理节点上绑定的端口是十分困难的，这也是NodePort类型的 Service 的缺点之一。而一旦 Pod 不再“借用”节点的 IP 和端口来暴露自身的服务，就不得不面临一个棘手的问题：Pod 的本质是节点中的进程，节点外的物理网络设备（交换机/路由器）并不知晓 Pod 的存在。它们在接收目的地址为 Pod IP 的数据包时，无法完成进一步的传输工作。
为此我们需要使用一些 CNI（Container Network Interface）插件（如 Flannel、Calico 和 Open vSwitch 等）来优化 Kubernetes 集群的网络模型，这种新型的网络设计理念称为 SDN（Software-defined Networking）。根据 SDN 实现的层级，我们可以将其分为 Underlay Network 和 Overlay Network：
Overlay 网络允许设备跨越底层物理网络（Underlay Network）进行通信，而底层却并不知晓 Overlay 网络的存在。Underlay 网络是专门用来承载用户 IP 流量的基础架构层，它与 Overlay 网络之间的关系有点类似物理机和虚拟机。Underlay 网络和物理机都是真正存在的实体，它们分别对应着真实存在的网络设备和计算设备，而 Overlay 网络和虚拟机都是依托在下层实体使用软件虚拟出来的层级。
Underlay Network 利用 Underlay Network 实现 Pod 跨节点通信，既可以只依赖 TCP/IP 模型中的二层协议，也可以使用三层。但无论哪种实现方式，都必须对底层的物理网络有所要求。</description></item><item><title>Open vSwitch</title><link>https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/</link><pubDate>Wed, 04 Nov 2020 09:19:42 +0100</pubDate><guid>https://koktlzz.github.io/kubernetes/kubernetes%E8%BF%9B%E9%98%B6/openvswitch/</guid><description>前言 Openshift SDN 是由 Overlay 网络 OVS（Open vSwitch）建立的，其使用的插件如下：
ovs-subnet: 默认插件，提供一个扁平化的 Pod 网络以实现 Pod 与其他任何 Pod 或 Service 的通信； ovs-multitenant：实现多租户管理，隔离不同 Project 之间的网络通信。每个 Project 都有一个 NETID（即 VxLAN 中的 VNID），可以使用 oc get netnamspaces 命令查看； ovs-networkpolicy：基于 Kubernetes 中的 NetworkPolicy 资源实现网络策略管理。 在 Openshift 集群中的节点上，有以下几个网络设备：
br0：OpenShift 创建和管理的 OVS 网桥，它会使用 OpenFlow 流表来实现数据包的转发和隔离； vxlan0：VxLAN 隧道端点，即 VTEP（Virtual Tunnel End Point），用于集群内部 Pod 之间的通信； tun0：节点上所有 Pod 的默认网关，用于 Pod 与集群外部和 Pod 与 Service 之间的通信； veth：Pod 通过veth-pair连接到br0网桥的端点。 ovs-ofctl -O OpenFlow13 show br0 命令可以查看br0上的所有端口及其编号：</description></item></channel></rss>